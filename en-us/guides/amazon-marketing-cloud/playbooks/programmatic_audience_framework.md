---
title: Amazon Marketing Cloud - Programmatic Audience Framework Playbooks
description: Programmatic Audience Framework
type: guide
interface: api
---
# Programmatic audience framework

Through this playbook, you will learn how to quickly and easily create multiple similar custom AMC audiences, monitor the refresh cadence, and measure the performance of your ad spend for these audiences.

Creating many AMC audiences instead of one large audience offers the advantage of precise measurement of line item performance. For instance, if a custom AMC audience is created with ASINs of shoppers that “*added to cart but didn't check out”*, it allows for clear measurement of each audience's effectiveness. Larger audiences containing many ASINs can make it more complex to measure.

The playbook will cover audience creation, measurement, optimization, and analysis using custom AMC audiences and the Amazon Demand Side Platform (Amazon DSP).

## What is programmatic audience framework?

Programmatic audience framework offers automation for the time-consuming and tedious aspects of the audience creation process when done at scale, with a goal of helping you identify the most effective audiences for continued media activation.
See table below for a breakdown by steps in the process:


| Step          | Description                                                                  | Business question                                                | Painpoint                                                         | Action/solution                                                                                                                                                  |
| --------------- | ------------------------------------------------------------------------------ | ------------------------------------------------------------------ | ------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Pre Campaign  | Create and monitor audience creation                                         | How do I quickly create custom AMC audiences?                    | Manual and repetitive API calls and monitoring                    | Create custom AMC audiences for activation in the Amazon DSP. Formulate programmatic audience creation via custom audiences API.                                 |
|               |                                                                              | How do I monitor which audiences are ready for activation?       |                                                                   |                                                                                                                                                                  |
|               |                                                                              | What should the logic of the audiences be?                       |                                                                   |                                                                                                                                                                  |
| Mid Campaign  | Refresh audiences over time and establish workflows to monitor performance   | How do I monitor my audiences during the course of the campaign? | Monitoring audience refreshes measuring many audiences            | Adjust spend based on audience performance and refresh cadence cadence. Perform refreshes every 30 days that update audience sizes based on successful refreshes |
| Post Campaign | Measure overall performance over advertising campaign generated by audiences | Which audience were most effective?                              | Non-templatized reports with little context to the bigger picture | Deploy dashboards that display overall performance volume and ROAS.                                                                                              |
|               |                                                                              | Which audiences should I continue to use?                        |                                                                   |                                                                                                                                                                  |

> [NOTE] This playbook does not utilize advanced measurements that can be performed to evaluate audience performance. Such use cases are described in the [multi-touch attribution (MTA)](guides/amazon-marketing-cloud/playbooks/MTA) and [customer life-time value (CLTV)](guides/amazon-marketing-cloud/playbooks/CLTV/) playbooks.

## Benefits

The instructions in the playbook allows for custom creation of large AMC audiences that can be monitored for activation through the course of the campaign.

## Prerequisites

- **Technical**

  - [AMC Insights on AWS](https://aws.amazon.com/solutions/implementations/amazon-marketing-cloud-insights-on-aws/?did=sl_card&trk=sl_card) is deployed and Work  flow manager (WFM) or a similar solution is configured.
  - **AMC concepts**, specifically data aggregation thresholds, privacy limitations, and other restrictions listed in the [AMC documentation](https://advertising.amazon.com/API/docs/en-us/guides/amazon-marketing-cloud/overview).
  - **Knowledge of [AMC SQL (link only available for AMC UI users)](https://advertising.amazon.com/marketing-cloud/instructional-queries/f53b1ad1359ef811fcdccdc8c6e69422b83bf06a163da53fddf79b6ca729edb6).** The processes outlined here relies on altering SQL code to match your purposes.
  - **Basic Python knowledge** to alter scripts outlined here to adapt to your use case for visualizing data for discovery of insights.
  - **Basic AWS Knowledge** of  [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html), and [Amazon Athena](https://docs.aws.amazon.com/athena/latest/ug/what-is.html) in specific. You will need to know how to set up an Amazon SageMaker Notebook in accordance with your company's standards.
  - **Basic knowledge of Amazon DSP** to activate upon  Audience Segmentation in the advertising market based on performance. You will need to know how to query audiences to activate in Amazon DSP.
  - **Knowledge required to build the data flow  pipeline** to run the automation of audience optimization with **familiarity of the [AMC APIs](guides/amazon-marketing-cloud/get-started/get-started) on Amazon Ads.**
- **Marketing**

  - **Understanding of Amazon DSP audience  segments** and how they are built, and the required logic to build new audiences.
  - **Data-driven decision-making and performance measurement** and a penchant for turning insights into action with media activations from AMC performance measures.
  - **Access to Amazon DSP and an ability to create a campaign  and line item with audiences**.

## Assumptions

We assume that you have an understanding of the process to create multiple audiences by submitting multiple API calls and track the audience creation manually, which is described in the [Rule based audience creation.](guides/amazon-marketing-cloud/audiences/rule-based-audiences)

> [NOTE] Rules-based audiences require a few constraints for the audiences to meet the size minimum requirements to be activated in the  marketplace. Currently, the minimum is 2000 distinct `user_ids` for a  rules-based audience to be activated in the market. Additionally, when  activating the audiences, there are metrics for affinity and audience  size, but that does not necessarily indicate an overlap between  audiences.

**Costs associated with implementing this playbook**

1. Sagemaker
   - At the time of publication, ml.t3.medium  is $0.05 per hour or $36 a month if always running. See the AWS docs here: [Amazon  SageMaker Pricing - Machine Learning - Amazon Web Services](https://aws.amazon.com/sagemaker/pricing/) for details on pricing.
   - The cost  will increase if larger compute options are chosen.
2. Submitting an audience to be created is not charged. There is no cost until the audience is activated.

## Playbook steps

The playbook is structured in the following order:

1. Introduces the concept of audience optimization and  approaches to measure audiences during pre, mid, and post campaign  reporting.
2. Defines the theory behind an ongoing audience optimization framework, and contexts in which it applies.
3. Outlines the prerequisites needed to build  optimization framework.
4. Provides a solution architecture build for a  pre-campaign audience build, mid-campaign audience optimization, and post  campaign audience reporting.
5. Outlines the next steps and provides more sophisticated measurement approaches.

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_1.png)

## Pre-campaign - formulate and identify

The pre-campaign section will cover how to create custom audiences via API, how to formulate the audience to be created, how to check which audiences are likely to be viable. The section will include guidance on how to formulate a question, a sample use-case, and both SQL and Python code.

### Formulate business question or hypothesis and create an audience

#### Rule-based audiences

The following steps will guide you on how to select ASINs, create custom AMC audiences and measure performance. The first step, however, is to formulate a business question that you want to address. The playbook will use a sample business question of “Users who added-to-cart but never checked out, which ASIN is most valuable to target about that product”. Based on the business question you select, you will then need to convert it to an SQL query.

**Other potential business questions or hypothesis**:

- Endemic:
  - Users that added to cart, but never purchased.
  - Users that viewed the detail page multiple times, but never purchased.
  - Users that read reviews, but never purchased.
  - Users that added to wishlist, but never purchased.
  - Users that searched for certain keywords, but never purchased.
  - Users that clicked on an ad, but did not purchase.
  - Users that repurchased an ASIN several times, but have not signed up for subscribe & save (SnS)
- Endemic and non-endemic
  - Users that have used SnS but not purchased another ASIN from the brand
  - Users that purchased at the last seasonal event. Ex: Christmas, Prime Day, Halloween.
  - Users that purchased ASIN(s) a certain amount of time ago, and might be in the time-frame to repurchase.
  - Users that have purchased over a threshold and have not purchased recently.
- Non-endemic
  - User that have clicked on an Amazon DSP ad, but never converted using first-party upload data

#### Rule-based lookalike audiences

The difference between a [rule-based lookalike audience](guides/amazon-marketing-cloud/audiences/rule-based-lookalike) and a normal [rule-based audience ](guides/amazon-marketing-cloud/audiences/rule-based-audiences)is that the lookalike audience will find users who *are similar to* the users sent in. This opens up new use cases as listed below:


| Use Case                                         | Description                                                                    | Metrics to focus on                                                                                | Best for                                                                 |
| -------------------------------------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------- |
| High long\-term value customers                  | Customers who contributed substantial sales to your brand                      | Total amount spend                                                                                 | All types of campaigns                                                   |
| Frequent buyers / Subscribe\-and\-Save customers | Customers who frequently purchased from your brand                             | Total number of purchases, total units of purchases                                                | All types of campaigns                                                   |
| New\-to\-brand (NTB) buyers                      | Customers who purchased recently for the very first time                       | NTB metrics                                                                                        | All types of campaigns                                                   |
| Brand loyalists                                  | Customers who made purchases across products or/and categories from your brand | Number of unique ASINs purchased                                                                   | Promoting new products, new product lines, and variety packs             |
| Bulk buyers                                      | Customers who purchased in bulk from you in the past                           | Number of units / order                                                                            | Promoting bundles, value packs, wholesale, etc                           |
| Seasonal shoppers                                | Customers who purchased during seasonal events previously                      | Dollars spent, number of purchases, or/and number of units purchased within particular time window | Seasonal campaigns                                                       |
| Offline buyers                                   | Customers who prefer to purchase from brick\-and\-mortar locations             | Offline conversion events                                                                          | Campaigns with mentions on offline presence or offline call\-to\-actions |
| High return\-on\-investment \(ROI\) customers    | Customers with measured high return on investment                              | Return on ad\-spend \(ROAS\), acquisition cost                                                     | Marketing executions with clear budget constraints                       |

Rule-based audiences are a better fit for situations where specific user actions are being selected. For example: “*Users that added to cart but never checked out*”. This is a strong fit for rule-based audiences as those specific users could potentially be the audience.

> [NOTE] Lookalike audiences contain NTB customers, including those not exposed to an ad. Lookalike audiences can be compared to other mid/upper funnel audiences, like in-market & lifestyle. We do not recommend comparing retargeting tactics with lookalike audiences as they reach customers at different points of their journey.

To create a lookalike audience instead of a rule-based audience, the only change is a different API endpoint with one additional parameter `(lookalikeAudienceExpectedReach`). For details, see [Rule-based lookalike audience ](https://advertising.amazon.com/API/docs/en-us/amc-rba#tag/Rule-based-audience-lookalike/operation/createLookalikeAudience). The monitoring endpoints remain the rule-based audience endpoints. The playbook supports both audiences via the parameter `is_lookalike`.

#### Recommended Flexible shopping insights datasets

One powerful source of input is incorporating the **Flexible shopping insights** (FSI) data for a complete view of conversions by adding those that were not driven by advertising, including complete SnS purchases for applicable brands. While ROAS can be calculated and activated upon as a rules-based audience without requiring a subscription to AMC’s **Paid features**, utilizing FSI can help tell a more complete story of the audience's behavior.

> [NOTE] Subscription to FSI provides access to the [conversions_all](guides/amazon-marketing-cloud/datasources/conversions_all_paid) table.

### Identify ASINs

This first step is exploratory and aims to identify high-value ASINs while minimizing audiences that may not perform well. This initial query will mirror the Amazon DSP audience creation query and the time range for the following steps. The starter query below returns ASINs with users who added items to their shopping cart but didn't make a purchase.

> [NOTE] To create a rule-based audience, a minimum of 2,000 unique user\_ids is required, but it's recommended to exceed this value as audiences will refresh, and user\_ids may change over time. The suggested minimum is 2,500 distinct users, but more users provide a higher cushion and larger sample size for measurement.

Run the following query in your AMC instance:

<details class="details-bar">
  <summary>Click to see script: users who added to cart and did not purchase</summary>

```
-- Find users who added to cart and did not purchase. 
-- Build the table that will find users who added to cart
WITH add_to_cart AS (
  SELECT
    user_id,
    tracked_item
  FROM conversions_all
WHERE
event_subtype = 'shoppingCart'
),
-- Find the users who completed and made the order
purchase_users AS (
  SELECT
    user_id,
    tracked_item
  FROM conversions_all
WHERE
-- could potentially adjust to also include sns subscription purchases as well 
event_subtype = 'order'
AND total_product_sales > 0
)
-- Take the add to cart users and left join with the users who purchased
SELECT
  a.tracked_item,
  COUNT(DISTINCT a.user_id) AS distinct_users
FROM add_to_cart a
  LEFT JOIN 
  purchase_users p 
  ON ((a.user_id = p.user_id) AND (a.tracked_item= p.tracked_item))
WHERE p.user_id IS NULL
GROUP BY 1
-- only show the tracked asins that have an add to cart user count of 5000 or above
HAVING COUNT(DISTINCT a.user_id) > 5000

```

</details>

With a sample output of:


| tracked\_item | distinct\_users |
| --------------- | ----------------- |
| ASIN1         | 10,000          |
| ASIN7         | 8,000           |
| ASIN52        | 6,000           |

This list of ASINs shows that ASIN1 has 10,000 users that “added to cart but never checked out”. A custom AMC audience could be created to reach these 10,000 users with an ad for ASIN1.

**Other Options for ASINs**

Groups of ASIN can be used instead of individual ASINs. Some examples of ASIN groupings are:

* Top sellers
* Gateway ASINs
* Loyalty ASINs
* Worst-selling ASINs
* Seasonal ASINs
* Strategic ASINs
  * High margin
  * New products or product lines
* Prime Day ASINs

### Data engineering

**Reference architecture**

See the image below for a reference architecture to help with the deployment. The architecture assumes that the [Amazon Ads API authentication based on OAuth 2.0](guides/account-management/authorization/overview#authorization-flow) is being used. The code samples provided in the following segments are designed for prototyping and to be referenced for a production deployment.

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_2.png)

#### Audience API access - Amazon Ads API

For this approach, you will need to use your `clientId`, `ClientSecret` and `refreshToken` to generate an access token and pass values via headers.

> [NOTE] Handle your credentials securely. In this example, [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/) is used to store credentials, but any credential management tool will work.

#### Set audience configuration

> [WARNING] A single set of audience will be created for each element of `AsinList` below. Ensure the list is of reasonable length and are all valued audiences. Know that there is no charge for creating an audience but audiences cannot be deleted or modified, so this could quickly clutter your Amazon DSP custom audiences. The default value for max `AsinList` is 25 but that can be modified.

<details class="details-bar">
  <summary>Click to see: script for audience</summary>

```
### Configurations
#Region - overwrite with new region if needed
region = boto3.session.Session().region_name

#Naming convention for Audience Names
#By default the namign conventions will follow: "Values entered by user below"[ASIN]
#Example MyAsinAudiences_ASIN1
audienceNamingFormat = "PlaybookAsinAudiences_"

#List of ASINs to deploy the audience
# See playbook for methods of generating this list
AsinList = ['ASIN15', 'Asin4', 'Asin5']

#AMC Instance Number - See AMC Audience API documentaion for reference photos of where to find this
instance_id = 'amcaaaaaaaa'

#DSP Advertiser ID to deploy audience
advertiserId = '11111111111'

### URL for Rule Based Audience - US Check documentation depending on region
endpointRBA = 'amc/audiences/query'

### Rule Based Lookalike URL - US Check documentation depending on region
endpointLaL = 'amc/audiences/lookalike'


### Lookalike audience Flag - True = This will be a lookalike audience. False = Rule based audience
is_lookalike = False

```

</details>

#### Headers for API authorization

<details class="details-bar">
  <summary>Click to see: script for API authorization</summary>

```
 ### Base URL FOR Rule Based Audience - US Check documentation depending on region
url = 'https://advertising-api.amazon.com/'

EntityID = 'ENTITYAAAAAA'
secrets_name = 'secrets_name_here'

### Pull API Gateway Credentials from secure storage - AWS Secrets manager in this scenario
api_gateway_credentials = pull_apigateway_secrets(secrets_name = secrets_name, region = region)
api_gateway_credentials['EntityID'] = EntityID

### Assumed Dictionary for api_gateway_credentials:
# {
#     'Clientid': 'amzn1.application-oa2-client.1111111111111111111', 
#     'Clientsecret': 'amzn1.oa2-cs.v1.222222222222222222222222222222',
#     'refreshToken': 'Atzr|yyyyyyyyyyyyyyy'
# }


#These credentials will be valid for an hour
apigw_headers = generate_headers(api_gateway_credentials, url, instance_id)

```

</details>

**Set parameter dictionary to pass to functions and save as .json**

<details class="details-bar">
  <summary>Click to see: script set parameter dictionary</summary>

```
# Define parameters for API interactions and configuration
params = {
    'EntityID': EntityID,  # ID of the entity
    'apigw_headers': apigw_headers,  # Headers for API Gateway requests
    'advertiserId': advertiserId,  # ID of the advertiser
    'instance_id': instance_id,  # ID of the instance
    'region': region,  # AWS region
    'url': url,  # URL for API requests
    'secrets_name': secrets_name,  # Name of the secret for accessing sensitive data
    'endpointLaL': endpointLaL,  # Endpoint for lookalike audience requests
    'endpointRBA': endpointRBA,  # Endpoint for RBA (rule-based audience) requests
    'audienceNamingFormat': audienceNamingFormat,  # Naming format for audiences
    'is_lookalike': is_lookalike  # Flag indicating whether the audience is a lookalike audience
}

# Save params without sensitive data
params_to_save = params.copy()  # Create a copy of the params dictionary
params_to_save.pop('apigw_headers')  # Remove sensitive data (API Gateway headers)
with open(f'configurations/{audienceNamingFormat}_params.json', 'w') as f:
    json.dump(params_to_save, f)  # Save the modified params dictionary as a JSON file

```

</details>

#### Set query detail

See the [AMC documentation](docs/en-us/amc-rba) for detailed descriptions on each of these parameters below. Note `query`, `audienceName` and `audienceDescription` are all required fields are not present in the code block below, they will be added to the configuration in following steps:

<details class="details-bar">
  <summary>Click to see: script for audience creation</summary>

```
# Fill out with Audience values
# See AMC documentation for details about parameters
body = {}
body['url'] = url  # URL for API requests
body['region'] = region  # AWS region
body['instance_id'] = instance_id  # ID of the instance
body['advertiserId'] = advertiserId  # ID of the advertiser
body['audienceNamingFormat'] = audienceNamingFormat  # Naming format for audiences
body['timeWindowStart'] = "2023-10-01T00:00:00Z"  # Start time of the time window
body['timeWindowEnd'] = "2023-10-31T00:00:00Z"  # End time of the time window
body['refreshRateDays'] = 7  # Refresh rate for the audience data (in days)
body['timeWindowRelative'] = "True"  # Indication of whether the time window is relative

# NOTE: Query value, audience name, and audience description are required and are set in the following cells
# body['query'] = "Set Later"
# body['audienceName'] = 'Set Later'
# body['audienceDescription'] = 'Set Later'

# If planning to use lookalike, another parameter is required:
# https://advertising.amazon.com/API/docs/en-us/amc-rba#tag/Rule-based-audience-lookalike/operation/createLookalikeAudience
body['lookalikeAudienceExpectedReach'] = 'BALANCED'  # Expected reach for the lookalike audience

# Save configs
with open(f'configurations/{audienceNamingFormat}_configuration.json', 'w') as f:
    json.dump(body, f)  # Save the body dictionary as a JSON file

```

</details>

#### Create audiences

> [WARNING] Running the code below will result in new audiences. We recommend that you run this code with one ASIN only in the `AsinList` to test the query and ensure that the audience creates successfully.

The missing elements from the `body` query details above are populated here. Each ASIN being inserted into the `AudienceName`, `audienceDescription` and `query`.

<details class="details-bar">
  <summary>Click to see: script for creating audiences</summary>

```
audience_list = []  # Initialize an empty list to store created audiences

for Asin in AsinList:
    # Define SQL query to select users who added to cart but did not convert for a specific ASIN
    # Adjust SQL query to fit specific use-cases
    sql = f""" WITH add_to_cart AS (
        SELECT
            user_id,
            tracked_item
        FROM
            conversions_for_audiences
        WHERE
            event_subtype = 'shoppingCart'
    ),
    purchase_users AS (
        SELECT
            user_id,
            tracked_item
        FROM
            conversions_for_audiences
        WHERE
            event_subtype = 'order'
            AND total_product_sales > 0
    )
    SELECT
        DISTINCT a.user_id
    FROM
        add_to_cart a
        LEFT JOIN purchase_users p ON (
            (a.user_id = p.user_id)
            AND (a.tracked_item = p.tracked_item)
        )
    WHERE
        p.user_id IS NULL
        AND a.tracked_item = '{Asin}'
    GROUP BY
        1
    """
  
    # Update body dictionary with SQL query and other parameters for audience creation
    body['query'] = sql
    body['audienceName'] = audienceNamingFormat + Asin
    body['audienceDescription'] = f"Rule-based Audience generated via the audience optimization playbook for {audienceNamingFormat} and {Asin}"
  
    # Convert body dictionary into string format (required for API call)
    body_string = json.dumps(body)
  
    # Call amc_audience function to create the audience and append the response to the audience list
    audience_list.append(amc_audience(method='POST',
                                      params=params,
                                      is_lookalike=is_lookalike,
                                      body=body_string))
  
print(f"Number of audiences submitted for creation: {len(audience_list)}")  # Print the number of audiences submitted for creation

```

</details>

#### Audience creation fail safes

**Number of audiences is too high**

If the number of audiences in the `AsinList` > 25, then the script will throw an error to reduce the limit of audiences to be created. If this limit is too low, you will need to manually raise the threshold limit or remove the failsafe.

`AsinList = asin_failsafe(AsinList, 25)`

will produce the error:

`Exception: Asin list is above the threshold value of: 25 this will create 50 unique audiences. If this is intentional, modify library to raise limit`

**Check for duplicate audiences**

<details class="details-bar">
  <summary>Click to see: script to check for duplicate audiences</summary>

```
existing_audience_name_list = pd.DataFrame(amc_audience(method = 'GET', params = params)['executionMetadata'])['audienceName'].to_list()
duplicated_audience_failsafe(existing_audience_name_list = existing_audience_name_list, AsinList = AsinList, audienceNamingFormat = audienceNamingFormat)
```

</details>

If the naming convention of `audienceNamingFormat` + Asin already exists in the DSP audiences, the following error is shown:

` Exception: Some of the audience names already exist in AMC. The existing  audiences are: PlaybookAsinAudiences_ASIN2, PlaybookAsinAudiences_ASIN1`

> [NOTE] We recommend that for each new use case of this playbook, use a new and unique `audienceNamingFormat` to allow for keeping the different test runs separate. If this is a modification to an existing run, remove this check, but “data persistence” will need to be manually updated to include the new audienceIDs.

#### Monitor audiences in AMC

Run the code block below to monitor the audiences created in the previous step. This block should be re-run until all audiences are either “SUCCEEDED” or “FAILED”. This will usually take several hours.

<details class="details-bar">
  <summary>Click to see: script to monitor created audiences</summary>

```
monitor_list = []
for audience in audience_list:
    monitor_list.append(amc_audience(method = 'GET', params = params, audienceExecutionId = audience['audienceExecutionId']))
  
#Create dataframe with columns - "StatusReason" only in return when applicable. Preset to have all possible columns
df_monitor = pd.DataFrame(columns = ['advertiserId', 'audienceDescription', 'audienceExecutionId', 'audienceName', 'createTime', 'instanceId', 'query', 'refreshRateDays', 'status', 'statusReason', 'timeWindowEnd', 'timeWindowRelative', 'timeWindowStar', 'audienceCount', 'dspAudienceId', 'dspCanonicalId']) 
#populate with list of json
df_monitor = pd.concat([df_monitor, pd.DataFrame(monitor_list)])

### High level overview of instance creation status
fig = px.histogram(df_monitor, x = 'status', title = 'Number of audiences by Status')
fig.update_layout(yaxis_title = 'Number of instances')
fig.show()

#display relevant data
display(df_monitor[['audienceName', 'createTime', 'status', 'statusReason', 'audienceCount', 'dspAudienceId']])

```

</details>

A sample output might look something like:

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_3.png)


| audienceName                | createTime                  | status    | statusReason                                                        | audienceCount | dspAudienceId |
| ----------------------------- | ----------------------------- | ----------- | --------------------------------------------------------------------- | --------------- | --------------- |
| AMC MyAsinAudiences\_Asin1  | 2023\-07\-06T16:32:00\.328Z | FAILED    | The audience did not meet the minimum required size in Amazon DSP\. | N/A           | N/A           |
| AMC MyAsinAudiences\_Asin7  | 2023\-07\-06T16:32:00\.827Z | SUCCEEDED | N/A                                                                 | 140000        | 222222        |
| AMC MyAsinAudiences\_Asin52 | 2023\-07\-06T16:32:01\.172Z | SUCCEEDED | N/A                                                                 | 30000         | 1111111       |
| AMC MyAsinAudiences\_Asin56 | 2023\-07\-06T16:32:01\.172Z | RUNNING   | N/A                                                                 | N/A           | N/A           |

The final step is to save the outputs. The outputs are saved for reference when pulling mid and post-campaign metrics so the specific audiences created in this step can be automatically identified using `audienceExecutionId`.

<details class="details-bar">
  <summary>Click to see: script to save audiences for reference</summary>

```
### Save the audiences to .csv for future reference
 df_monitor.to_csv(f'{audienceNamingFormat}_records.csv')

```

</details>

### Insights and visualization

After the audiences have completed, the next step will be to visualize the state of the audiences.

**If needed, investigate failure codes**

<details class="details-bar">
  <summary>Click to see: script to identify failure codes</summary>

```
fig = px.histogram(df_monitor[df_monitor['status']=='FAILED'], x = 'statusReason', title = 'Number of audiences by failure reason')
fig.update_layout(yaxis_title = 'Number of instances')
fig.show()

### Drill into specifics:
df_monitor[df_monitor['status']=='FAILED']

```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_4.png)

See the [AMC audience API documentation](API/docs/en-us/amc-rba) for **additional details on error codes**. If the audience is failing to meet minimum required size, consider extending the time window, or using a logical grouping of ASINs, see “[Formulate Business Question](#formulate-business-question-or-hypothesis-and-create-an-audience)” for more details.

#### Review successful audiences

First see the histogram of audience size to get an idea of audience size.

<details class="details-bar">
  <summary>Click to see: script to check audience size</summary>

```
fig = px.histogram(df_monitor[df_monitor['status']=='SUCCEEDED'], x = 'audienceCount', nbins=10, title = 'Distribution of audience size')
fig

```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_5.png)

The final step is to view the top audiences, by size:

<details class="details-bar">
  <summary>Click to see: script to view top audiences, by size</summary>

```
fig = px.bar(df_monitor[df_monitor['status']=='SUCCEEDED'].sort_values(by='audienceCount', ascending=False), x = 'audienceName', y='audienceCount', title = 'Top Audiences by audience size')
fig

```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_6.png)

### Data persistence

Data will be persisted between stages from pre-campaign through to post-campaign using a .csv stored in your Sagemaker instance. This will primarily be used to track `audienceExecutionId` for monitoring of refresh cadence to ensure all audiences successfully update with new user\_ids.

### Data overlap

Before deploying your audiences, it’s recommended you measure the overlap they have with existing audience segments available in Amazon DSP. These can be found by measuring the overlap percentage of created audiences with existing audiences in the `conversions_all` table. This is recommended to avoid users being included twice as members of separate audience groups, which will extend reach. This can be done at the launch of the campaign, as well as set up as a mechanism during the mid-campaign audience refreshes to ensure that the refreshed audience groups don’t have a high overlap with another audience deployed in market.

### Monitor audiences in DSP

Once an audience creation status is returned as “successful” in the AMC API, it can take between 24 and 36 hours to be “active” and ready for use in Amazon DSP.

To track when they are available the code below will pull the list of audiences from the Amazon DSP API.

> [NOTE] Currently there is no unique id field that is common for both the AMC endpoint and Amazon DSP endpoint. This is a known issue and is being addressed. The other only way to map audiences at the moment is by name and by the approximate creation date (they are different values and cannot be used to join).

<details class="details-bar">
  <summary>Click to see: script to monitor audience activation status</summary>

```
### DSP specific parameter

### Note this value can be pulled from this endpoint: "/v2/profiles"
### https://advertising.amazon.com/API/docs/en-us/reference/2/profiles

AdvertisingAPIScope = '0000000000000'

# Add in one new required headers parameter
apigw_headers['Amazon-Advertising-API-Scope'] = AdvertisingAPIScope

```

</details>

To make the API call:
Note this will return a very large json object. It is converted into a pandas dataframe then filtered to:

1. Custom audiences from AMC
2. Names with the same naming format set in “Set audience configurations”

<details class="details-bar">
  <summary>Click to see: script to list all DSP audiences</summary>

```
### The following call will list all DSP Audiences under the category of "Custom-built"

body = {"adType": "DSP", 
        "filters": 
        [
            {"field": "category", "values": ["Custom-built"]
            }
        ]
    }

r = requests.post(f"https://advertising-api.amazon.com/audiences/list?advertiserId={advertiserId}&maxResults=250", headers = apigw_headers , data = json.dumps(body))
print(r)

if r.ok:
    print("Request successful")
    # Convert values returned from API into a pandas dataframe for easier parsing

    df = pd.DataFrame(r.json()['audiences'])
    df = df[df["subCategory"]=='AMC'].sort_values(by= 'createDate', ascending = False)
    df = df[df['audienceName'].str.startswith(f'AMC {audienceNamingFormat}')]
    display(df)

else:
    print("Error in API call")
    print(r.text)

```

</details>

Example output:


| audienceId          | audienceName                     | description                                          | category      | subCategory | createDate                  | updateDate                  | status | forecasts                      | fees                                                   | providerId      |
| --------------------- | ---------------------------------- | ------------------------------------------------------ | --------------- | ------------- | ----------------------------- | ----------------------------- | -------- | -------------------------------- | -------------------------------------------------------- | ----------------- |
| 1111111111111111111 | AMC PlaybookAsinAudiences\_ASIN1 | Rule based Audience generated via the audience\.\.\. | Custom\-built | AMC         | 2023\-11\-15T22:34:22\.698Z | 2023\-11\-16T16:52:51\.896Z | Active | \{'inventoryForecasts': \{\}\} | \[\{'amount': 150000, 'currency': 'USD', 'scale'\.\.\. | 222222222222222 |

### Action

The next step is to go to the DSP web console and create campaigns and line items on these custom rule-based audiences. The following sections of this playbook will cover how to monitor performance and data refresh of these campaigns during and post campaign. There are [API endpoints for creating line items](reference/dsp/dsp-campaign-management-overview), you can automate creation of line items with your team, but that will not be done in this playbook.

## Mid campaign - monitor and adjust

Audiences once created successfully will attempt to update on the schedule defined in the API call, which could be between 1 and 30 days. Every defined number of days, the list of users in the audience will update. The update could potentially cause a lower number of audiences being created as compared to the audiences that was created at the first instance, which could also mean that the number drops below the required user threshold when future scheduled updates are performed.
For example: Users who added to cart but never checked out might have 5,000 users on audience creation date; but after 4 weeks only the number could be 1,000 users; which is below the threshold for a rule-based audience.

Since the API does not directly report which audiences that created successfully but are now below the user threshold, we will use the instructions in this section to derive and monitor the audiences and accordingly adjust. Once an audience out-of-date for more than 30 days, it will get deactivated in the DSP.

### Step 1: Pull in the audiences IDs and configs from the data store

<details class="details-bar">
  <summary>Click to see: script to import audiences</summary>

```
### Import data from data store 
params = json.load(open("configurations/PlaybookAsinAudiences_.json"))
# display(params)
audienceNamingFormat = params['audienceNamingFormat']


df = pd.read_csv(f"configurations/{audienceNamingFormat}_records.csv")
# display(df.head())

configs = json.load(open(f"configurations/{audienceNamingFormat}_configuration.json"))
# display(configs)

```

</details>

### Step 2: Monitor audiences to build query

<details class="details-bar">
  <summary> Click to see: script to create list of audiences to monitor </summary>

```
# Create list of audiences to monitor from data store
audience_list = df['audienceExecutionId'].tolist()
audience_list

```

</details>

Sample output:

```
['11111111-1111-1111-1111-111111111111',
 '...']
```

Check to see if the number of days since the last refresh is greater than the refresh cadence. Note that there is a one-day buffer built into the sample code. This code will output all audiences that have not refreshed on cadence. While more investigation could be required to understand why a set of audience has not refreshed, it is most likely the audience size has dropped below 2,000 unique users.

<details class="details-bar">
  <summary>Click to see: script to output all audiences that have not refreshed on cadence</summary>

```

 # Initialize an empty list to store monitor data
monitor_list = []

# Set Pandas option to display all columns
pd.set_option('display.max_columns', None)

# Iterate through each audience in the audience_list
for audience in audience_list:
 # Call amc_audience function with specified parameters and append the result to monitor_list
    monitor_list.append(amc_audience(method = 'GET', params = params, audienceExecutionId = audience))

#Create dataframe with columns - "StatusReason" only in return when applicable. Preset to have value
df_monitor = pd.DataFrame(columns = ['advertiserId', 'audienceDescription', 'audienceExecutionId', 'audienceName', 'createTime', 'instanceId', 'query', 'refreshRateDays', 'status', 'statusReason', 'timeWindowEnd', 'timeWindowRelative', 'timeWindowStart', 'audienceCount', 'dspAudienceId', 'dspCanonicalId', 'lastRefreshedTime']) 
#populate with list of json
df_monitor = pd.concat([df_monitor, pd.DataFrame(monitor_list)])

try:
 # Filter DataFrame to include only rows with 'SUCCEEDED' status
    df_monitor = df_monitor[df_monitor['status']=='SUCCEEDED']
     # Calculate time since last refresh in days
    df_monitor['TimeSinceLastRefresh'] =  (date.today() - pd.to_datetime(df_monitor['lastRefreshedTime']).dt.date)
    df_monitor['TimeSinceLastRefreshInt'] =  df_monitor['TimeSinceLastRefresh'].apply(lambda x: x.days)

    #1 Day buffer - if true dataset has not refreshed on schedule or refresh schedule is 0
    df_monitor['AudienceOutOfDate'] = df_monitor['TimeSinceLastRefreshInt']-df_monitor['refreshRateDays'] > 1
    display(df_monitor)

except Exception as e:
 # Handle any exceptions that may occur and print an error message
    print("error occured ", e)
     # Display the DataFrame even if an error occurs
    display(df_monitor)

```

</details>

**Sample output**:


| audienceName                     | createTime               | instanceId  | query                                                | refreshRateDays | status    | audienceCount | dspAudienceId | lastRefreshedTime        | TimeSinceLastRefresh | TimeSinceLastRefreshInt | AudienceOutOfDate |
| ---------------------------------- | -------------------------- | ------------- | ------------------------------------------------------ | ----------------- | ----------- | --------------- | --------------- | -------------------------- | ---------------------- | ------------------------- | ------------------- |
| AMC PlaybookAsinAudiences\_ASIN1 | 2023-07-07T16:55:05.183Z | amc11111111 | SELECT user\_id from conversions\_for\_audiences ... | 5               | SUCCEEDED | 10000         | 2415475       | 2023-07-07T18:52:07.823Z | 8 days               | 8                       | TRUE              |

**Failed audience action**

If an audience has failed to refresh for several refresh states, it is recommended to decrease or stop spending on that as the line item as it will be continuing to target the same audience even after it has moved down the funnel. This would likely impact the rate of lower funnel conversions coming from that targeted audience.

### Step 3: Query campaign performance from AMC

**Campaign and line item**:

After the custom AMC audience has been tagged to a campaign and has had sufficient time to run, the following steps will build a workflow to monitor and optimize the query. To see preliminary results, a minimum of one week is the recommended time for the campaign.

**Audience optimization query**:

Within WFM via AMC Insights on an AWS deployed notebook, create a new workflow using the query below.
The query is designed to output measurement insights from the data into the data lake on a schedule for updated insights. The query used is an audience overlap percentage query that assesses the incremental reach and conversion percentage of different audience segments.

The first step will be to build a query within AMC that will provide audience performance data. This audience can run directly in the AMC UI over a time period within 13 months and it will output a finding to confirm the query is valid.

<details class="details-bar">
  <summary>Click to see: script for audience optimization</summary>

```
-- Common Table Expressions (CTEs) to process and filter data
WITH 
traffic AS (
    -- CTE to select relevant impressions data from a specified time window
    SELECT 
        advertiser, 
        advertiser_id, 
        campaign, 
        campaign_id, 
        behavior_segment_name, 
        user_id, 
        impression_dt_utc as event_dt_utc, 
        impressions
    FROM TABLE ( 
        EXTEND_TIME_WINDOW(
            'dsp_impressions_by_matched_segments', 
            'P14D', 
            'P0D'
            ) 
        ) 
    WHERE ( impressions = 1 ) 
),

matched_and_ranked AS (
    -- CTE to join and rank conversion events based on a specified time window
    SELECT 
        t.advertiser, 
        t.advertiser_id, 
        t.campaign, 
        t.campaign_id, 
        c.user_id, 
        t.behavior_segment_name, 
        c.event_dt_utc, 
        c.conversion_id, 
        c.event_subtype, 
        c.event_category, 
        CASE 
            WHEN c.event_subtype = 'detailPageView' THEN c.conversions 
            ELSE 0 
        END AS DPV, 
        CASE 
            WHEN c.event_subtype = 'shoppingCart' THEN c.conversions 
            ELSE 0 
        END AS ATC, 
        CASE 
            WHEN c.event_category = 'purchase' THEN c.conversions 
            ELSE 0 
        END AS Purchases, 
        ROW_NUMBER() OVER (
            PARTITION BY conversion_id, behavior_segment_name 
            ORDER BY SECONDS_BETWEEN(
                t.event_dt_utc, 
                c.event_dt_utc
                ) ASC 
            ) AS match_rank, 
        impressions 
    FROM conversions_all c 
    INNER JOIN traffic t 
        ON c.user_id = t.user_id 
    WHERE SECONDS_BETWEEN(
            t.event_dt_utc, 
            c.event_dt_utc
            ) BETWEEN 0 AND 14 * 24 * 60 * 60 
        AND ( 
            event_subtype = 'detailPageView' 
            OR event_subtype = 'shoppingCart' 
            OR event_category = 'purchase' 
            ) 
),

attributed_conversions AS (
    -- CTE to calculate attributed conversions based on match rank and impressions
    SELECT 
        advertiser, 
        advertiser_id, 
        campaign, 
        campaign_id, 
        behavior_segment_name, 
        event_dt_utc,
        SUM(DPV) AS DPV, 
        SUM( 
            CASE 
                WHEN DPV = 1 AND impressions = 1 THEN DPV 
                ELSE 0 
            END 
        ) AS DPV_views, 
        SUM(ATC) AS ATC, 
        SUM( 
            CASE 
                WHEN ATC = 1 AND impressions = 1 THEN ATC 
                ELSE 0 
            END 
        ) AS ATC_views, 
        SUM(Purchases) AS Purchases, 
        SUM( 
            CASE 
                WHEN Purchases = 1 AND impressions = 1 THEN Purchases 
                ELSE 0 
            END 
        ) AS Purchases_views 
    FROM matched_and_ranked r 
    WHERE match_rank = 1 
    GROUP BY 1, 2, 3, 4, 5, 6
),

relevant_traffic AS (
    -- CTE to select relevant traffic data within a specified time window
    SELECT 
        advertiser, 
        advertiser_id, 
        campaign, 
        campaign_id, 
        behavior_segment_name, 
        event_dt_utc,
        SUM(impressions) AS impressions 
    FROM traffic 
    WHERE event_dt_utc >= BUILT_IN_PARAMETER('TIME_WINDOW_START') 
        AND event_dt_utc < BUILT_IN_PARAMETER('TIME_WINDOW_END') 
    GROUP BY 1, 2, 3, 4, 5, 6
)

-- Main query to calculate total impressions and conversions for relevant traffic
SELECT 
    t.advertiser AS advertiser_name, 
    t.advertiser_id AS advertiser_id, 
    t.campaign AS campaign_name, 
    t.campaign_id AS campaign_id, 
    t.behavior_segment_name AS segment, 
    t.event_dt_utc,
    SUM(COALESCE(t.impressions, 0)) AS impressions, 
    SUM(COALESCE(ac.DPV, 0)) AS Total_DPV, 
    SUM(COALESCE(ac.DPV_views, 0)) AS Total_DPV_views, 
    SUM(COALESCE(ac.ATC, 0)) AS Total_ATC, 
    SUM(COALESCE(ac.ATC_views, 0)) AS Total_ATC_views, 
    SUM(COALESCE(ac.Purchases, 0)) AS Total_Purchases, 
    SUM(COALESCE(ac.Purchases_views, 0)) AS Total_Purchases_views 
FROM relevant_traffic t 
INNER JOIN attributed_conversions ac 
    ON ac.behavior_segment_name = t.behavior_segment_name 
GROUP BY 1, 2, 3, 4, 5, 6
HAVING impressions > 1000 
    OR Total_DPV > 0 
    OR Total_ATC > 0 
    OR Total_Purchases > 0;

```

</details>

**WFM create workflow**:

This step will set up a performance feed of data from AMC into an S3 bucket of your AWS account. This will involve creating a workflow based on a SQL query and executing the workflow over a specified time frame to output the data into a data lake.

Define a workflow ID and add a single line query of the audience optimization workflow from AMC to output into the data lake.

<details class="details-bar">
  <summary>Click to see: script to create workflow</summary>

```
# Create a workflow

workflowDefinition = {
     "filteredMetricsDiscriminatorColumn": "filtered",
     "sqlQuery": "WITH traffic AS ( SELECT advertiser, advertiser_id, campaign, campaign_id, behavior_segment_name, user_id, impression_dt_utc as event_dt_utc, impressions FROM TABLE ( EXTEND_TIME_WINDOW( 'dsp_impressions_by_matched_segments', 'P14D', 'P0D' ) ) WHERE ( impressions = 1 ) ), matched_and_ranked AS ( SELECT t.advertiser, t.advertiser_id, t.campaign, t.campaign_id, c.user_id, t.behavior_segment_name, c.event_dt_utc, c.conversion_id, c.event_subtype, c.event_category, CASE WHEN c.event_subtype = 'detailPageView' THEN c.conversions ELSE 0 END AS DPV, CASE WHEN c.event_subtype = 'shoppingCart' THEN c.conversions ELSE 0 END AS ATC, CASE WHEN c.event_category = 'purchase' THEN c.conversions ELSE 0 END AS Purchases, ROW_NUMBER() OVER ( PARTITION BY conversion_id, behavior_segment_name ORDER BY SECONDS_BETWEEN( t.event_dt_utc, c.event_dt_utc ) asc ) AS match_rank, impressions FROM conversions c INNER JOIN traffic t ON c.user_id = t.user_id WHERE SECONDS_BETWEEN( t.event_dt_utc, c.event_dt_utc ) BETWEEN 0 AND 14 * 24 * 60 * 60 AND ( event_subtype = 'detailPageView' OR event_subtype = 'shoppingCart' OR event_category = 'purchase' ) ), attributed_conversions AS ( SELECT advertiser, advertiser_id, campaign, campaign_id, behavior_segment_name, event_dt_utc, SUM(DPV) AS DPV, SUM( CASE WHEN DPV = 1 AND impressions = 1 THEN DPV ELSE 0 END ) AS DPV_views, SUM(ATC) AS ATC, SUM( CASE WHEN ATC = 1 AND impressions = 1 THEN ATC ELSE 0 END ) AS ATC_views, SUM(Purchases) AS Purchases, SUM( CASE WHEN Purchases = 1 AND impressions = 1 THEN Purchases ELSE 0 END ) AS Purchases_views FROM matched_and_ranked r WHERE match_rank = 1 GROUP BY 1, 2, 3, 4, 5, 6 ), relevant_traffic AS ( SELECT advertiser, advertiser_id, campaign, campaign_id, behavior_segment_name, event_dt_utc, SUM(impressions) AS impressions FROM traffic WHERE event_dt_utc >= BUILT_IN_PARAMETER('TIME_WINDOW_START') AND event_dt_utc < BUILT_IN_PARAMETER('TIME_WINDOW_END') GROUP BY 1, 2, 3, 4, 5, 6 ) SELECT t.advertiser AS advertiser_name, t.advertiser_id AS advertiser_id, t.campaign AS campaign_name, t.campaign_id AS campaign_id, t.behavior_segment_name AS segment, t.event_dt_utc, SUM(COALESCE(t.impressions, 0)) AS impressions, SUM(COALESCE(ac.DPV, 0)) AS Total_DPV, SUM(COALESCE(ac.DPV_views, 0)) AS Total_DPV_views, SUM(COALESCE(ac.ATC, 0)) AS Total_ATC, SUM(COALESCE(ac.ATC_views, 0)) AS Total_ATC_views, SUM(COALESCE(ac.Purchases, 0)) AS Total_Purchases, SUM(COALESCE(ac.Purchases_views, 0)) AS Total_Purchases_views FROM relevant_traffic t INNER JOIN attributed_conversions ac on ac.behavior_segment_name = t.behavior_segment_name GROUP BY 1, 2, 3, 4, 5, 6 HAVING impressions > 1000 OR Total_DPV > 0 OR Total_ATC > 0 OR Total_Purchases > 0",
     "workflowId": "Audience_Optimization_Test"
   }

## Note: If you would like to automatically update the workflow within this method, you can change the Boolean value below to True
amc_response = wfm.create_workflow(workflowDefinition, False, )

#print the output
print(f"Request succeeded: {amc_response.success}")

for key in amc_response.response:
   print (f"{key}:\n{amc_response.response.get(key,'')}\n")

```

</details>

**Output**

```
Request succeeded: True
responseReceivedTime:
2023-07-24T19:24:38

responseStatus:
CREATED

```

**WFM execute Workflow**:
The next step will be to run the next API call that takes the created workflow and executes upon it over a specified time frame within the US data.

<details class="details-bar">
  <summary>Click to see: script to execute a workflow</summary>

```
# Execute a workflow

execution_request =  {
    "workflowId": "{{latestworkflowIdCreated}}",
    "timeWindowStart": "2023-07-16T00:00:00",
    "timeWindowEnd": "2023-07-23T00:00:00",
    "timeWindowType": "EXPLICIT",
    "timeWindowTimeZone": "America/New_York",
      "workflowId": "Audience_Optimization_Test"
  
}

create_workfow_amc_response = wfm.create_workflow_execution(execution_request)

workflow_execution_id = create_workfow_amc_response.response.get('workflowExecutionId','')

#print the output
print(f"Request succeeded: {create_workfow_amc_response.success}")

for key in create_workfow_amc_response.response:
    print (f"{key}:\n{create_workfow_amc_response.response.get(key,'')}\n")

```

</details>

**Output**:

```
Request succeeded: True
responseReceivedTime:
2023-07-24T19:54:35

responseStatus:
PENDING

amcVersionHash:
6141361052+c1938760-aeb7-4c6d-8788-173e72353ac5

availableTimeBeforeWindowSecs:
2505600

createTime:
2023-07-24T19:54:11Z

dataGaps:
[]

...

```

**Schedule workflow**:
Once the workflows are created and executed, the next step is to schedule a cadence for when this audience optimization query will repeat. The query below will repeat a new workflow run on a weekly basis every Tuesday at 5:00 UTC.

<details class="details-bar">
  <summary>Click to see: script to schedule a workflow</summary>

```
create_schedule_request =  {
    "scheduleId": "Audience_Optimization_Test-Sched",
    "workflowId": "Audience_Optimization_Test",
    "aggregationHourUtc": "5",
    "aggregationPeriod": "Weekly",
    "aggregationStartDay": "Tuesday",
      "scheduleEnabled": "true"
}

create_schedule_amc_response = wfm.create_schedules(create_schedule_request)
print(create_schedule_amc_response)

#print the output
print(f"Request succeeded: {create_schedule_amc_response.success}")

for key in create_schedule_amc_response.response:
    print (f"{key}:\n{create_schedule_amc_response.response.get(key,'')}\n")
```

</details>

**Output**:

```
<wfm_amc_api_interface.wfm_amc_api_interface.AMCAPIResponse object at 0x7f489360e630>
Request succeeded: True
responseReceivedTime:
2023-07-24T19:36:16

responseStatus:
200

```

**Athena output**

After the initial execution has successfully completed, the data will then be accessible in your AWS Athena table under the workflow ID assigned in the workflow creation step. As the schedule and backfill regularly inputs data into the data lake, this Athena table will be able to house the data in real time.

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_6.png)

The data is now available within your data lake and ready to begin building a workflow for ongoing monitoring.

### Step 4: Pull the data from the data lake into a notebook

Once the data is accessible in the AWS data lake, it is available for querying in your Sagemaker instance. The following steps will provide guidance to query data in your Sagemaker instance and to generate visuals out of the data to derive context on mid-campaign performance. The AMC query was designed to target a single audience per line item grouped by `advertiser_id`, `campaign_name`, `campaign_id`, and `event date`.

**Query AMC To get line item performance**

<details class="details-bar">
  <summary>Click to see: script to get line item performance</summary>

```
# Go to your WFM notebook and run
# Assume WFM has been deployed. Run this query. 

### Generate Query using Audience names from data store
## Query = WITH traffic AS ( SELECT advertiser, advertiser_id, campaign, campaign_id, behavior_segment_name, user_id, impression_dt_utc as event_dt_utc, impressions FROM TABLE( EXTEND_TIME_WINDOW('dsp_impressions_by_matched_segments', 'P14D', 'P0D') ) WHERE ( impressions = 1 ) ), matched_and_ranked AS ( SELECT t.advertiser, t.advertiser_id, t.campaign, t.campaign_id, c.user_id, t.behavior_segment_name, t.event_dt_utc, c.conversion_id, c.event_subtype, c.event_category, CASE WHEN c.event_subtype = 'detailPageView' THEN c.conversions ELSE 0 END AS DPV, CASE WHEN c.event_subtype = 'shoppingCart' THEN c.conversions ELSE 0 END AS ATC, CASE WHEN c.event_category = 'purchase' THEN c.conversions ELSE 0 END AS Purchases, ROW_NUMBER() OVER( PARTITION BY conversion_id, behavior_segment_name ORDER BY SECONDS_BETWEEN(t.event_dt_utc, c.event_dt_utc) asc ) AS match_rank, impressions FROM conversions c INNER JOIN traffic t ON c.user_id = t.user_id WHERE SECONDS_BETWEEN(t.event_dt_utc, c.event_dt_utc) BETWEEN 0 AND 14 * 24 * 60 * 60 AND ( event_subtype = 'detailPageView' OR event_subtype = 'shoppingCart' OR event_category = 'purchase' ) ), attributed_conversions AS ( SELECT advertiser, advertiser_id, campaign, campaign_id, behavior_segment_name, SUM(DPV) AS DPV, SUM( CASE WHEN DPV = 1 AND impressions = 1 THEN DPV ELSE 0 END ) AS DPV_views, SUM(ATC) AS ATC, SUM( CASE WHEN ATC = 1 AND impressions = 1 THEN ATC ELSE 0 END ) AS ATC_views, SUM(Purchases) AS Purchases, SUM( CASE WHEN Purchases = 1 AND impressions = 1 THEN Purchases ELSE 0 END ) AS Purchases_views FROM matched_and_ranked r WHERE match_rank = 1 GROUP BY 1, 2, 3, 4, 5 ), relevant_traffic AS ( SELECT advertiser, advertiser_id, campaign, campaign_id, behavior_segment_name, SUM(impressions) AS impressions FROM traffic WHERE event_dt_utc >= BUILT_IN_PARAMETER('TIME_WINDOW_START') AND event_dt_utc < BUILT_IN_PARAMETER('TIME_WINDOW_END') GROUP BY 1, 2, 3, 4, 5 ) SELECT t.advertiser AS advertiser_name, t.advertiser_id AS advertiser_id, t.campaign AS campaign_name, t.campaign_id AS campaign_id, t.behavior_segment_name AS segment, SUM(COALESCE(t.impressions, 0)) AS impressions, SUM(COALESCE(ac.DPV, 0)) AS Total_DPV, SUM(COALESCE(ac.DPV_views, 0)) AS Total_DPV_views, SUM(COALESCE(ac.ATC, 0)) AS Total_ATC, SUM(COALESCE(ac.ATC_views, 0)) AS Total_ATC_views, SUM(COALESCE(ac.Purchases, 0)) AS Total_Purchases, SUM(COALESCE(ac.Purchases_views, 0)) AS Total_Purchases_views FROM relevant_traffic t INNER JOIN attributed_conversions ac on ac.behavior_segment_name = t.behavior_segment_name GROUP BY 1, 2, 3, 4, 5 HAVING impressions > 1000 OR Total_DPV > 0 OR Total_ATC > 0 OR Total_Purchases > 0
df_analysis = pd.read_csv("audience_optimization_test.csv")
df_analysis

```

</details>


| advertiser\_name | advertiser\_id | campaign\_name | campaign\_id | event\_dt\_utc | segment                         | total\_spend | impressions | total\_dpv | total\_atc | total\_purchases | total\_sales | export\_month | file\_last\_modified |
| ------------------ | ---------------- | ---------------- | -------------- | ---------------- | --------------------------------- | -------------- | ------------- | ------------ | ------------ | ------------------ | -------------- | --------------- | ---------------------- |
| Brand1           | 12345          | campaign_1     | 1111111      | 7/26/2023      | AMC PlaybookAsinAudiences-ASIN1 | 2343         | 10828       | 224        | 94         | 67               | 23234        | 7             | 2023-07-24T21-27-02  |
| Brand1           | 12345          | campaign_2     | 1111111      | 7/25/2023      | AMC PlaybookAsinAudiences-ASIN1 | 4323         | 10798       | 224        | 94         | 67               | 34342        | 7             | 2023-07-24T21-27-02  |

### Step 5: Create visualizations of ongoing audience performance through the campaign

<details class="details-bar">
  <summary>Click to see: script to output visualization of audience peformance </summary>

```
#Chart1
import pandas as pd
import io
import seaborn as sns
import matplotlib.pyplot as plt

# Perform groupby operation on the DataFrame 'df_analysis' based on the column 'segment', 
# summing up all other numerical columns, and then resetting the index
df_analysis2 = df_analysis.groupby('segment').sum().reset_index()

# Chart 1: Create a bar plot for 'total_dpv' and 'total_atc' against 'segment'
df_analysis2.plot(x='segment',  # X-axis values from the 'segment' column
                  y=['total_dpv', 'total_atc'],  # Y-axis values from 'total_dpv' and 'total_atc' columns
                  kind='bar',  # Plot type is bar chart
                  figsize=(8, 5),  # Figure size (width, height) in inches
                  width=0.9,  # Width of the bars
                  color=('lightgreen', 'darkorange'))  # Colors for the bars

# Chart 2: Create a line plot for 'total_purchases' against 'segment'
df_analysis2.plot(x='segment',  # X-axis values from the 'segment' column
                  y=['total_purchases'],  # Y-axis values from 'total_purchases' column
                  kind='line',  # Plot type is line chart
                  figsize=(8, 5),  # Figure size (width, height) in inches
                  marker='*',  # Marker style for data points
                  color=('black'),  # Color of the line
                  ms=10)  # Marker size
# Customize plot appearance 
plt.xticks(rotation=30, ha='right') # Rotate x-axis labels by 30 degrees and align them to the right 
plt.title('Total conversions per segment', fontsize=15, fontweight='bold', pad=10) # Set title with font properties 
plt.legend(['total_purchases']) # Add legend with specified labels 
plt.xlabel('') # Remove x-axis label 
plt.show() # Display the plot

```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_8.png)
![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_9.png)

<details class="details-bar">
  <summary>Click to see: script to output visualization of audience peformance </summary>

```
from plotly.subplots import make_subplots

# Group the DataFrame 'df_analysis' by 'event_dt_utc' and 'segment', summing up all other numerical columns, and then reset the index
df_analysis3 = df_analysis.groupby([
    'event_dt_utc',
    'segment']).sum().reset_index()
# Calculate ROAS (Return on Advertising Spend)
df_analysis3['ROAS'] = df_analysis3['total_sales']/df_analysis3['total_spend']

# Convert 'event_dt_utc' column to datetime format
df_analysis3['event_dt_utc']= pd.to_datetime(df_analysis3['event_dt_utc'])

# Sort the DataFrame by 'event_dt_utc'
df_analysis3 = df_analysis3.sort_values(by=['event_dt_utc'])

# Create a figure with subplots using Plotly's make_subplots function
fig = px.line(df_analysis3,  # DataFrame to use
              x='event_dt_utc',  # X-axis values from 'event_dt_utc' column
              y=['ROAS', 'total_spend', 'impressions', 'total_dpv', 'total_purchases', 'total_sales'],  # Y-axis values for each subplot
              color=df_analysis3['segment'],  # Color by 'segment'
              facet_col=df_analysis3['segment'],  # Arrange subplots into columns based on 'segment'
              facet_col_spacing=0.04,  # Spacing between subplot columns
              facet_col_wrap=2,  # Number of subplot columns per row
              height=1000,  # Height of the entire figure
              title='ROAS')  # Figure title

# Update layout settings
fig.update_layout(
    showlegend=False,  # Hide legend
    autosize=True,  # Allow autosizing of the plot
    updatemenus=[  # Define update menus for switching between different metrics and plot types
        dict(
            active=0,  # Index of the initially active button
            buttons=list([  # List of buttons for different metrics
                dict(label="ROAS",  # Label for the button
                     method="update",  # Action method for the button
                     args=[{"visible": [True, False, False, False, False, False]},  # Visibility settings for traces
                           {"title": "ROAS"}]),  # Update title when button is clicked
                # Similar buttons for other metrics like 'Total Spend', 'Impressions', 'Total DPV', 'Total Purchase', 'Total Sales'
            ]),
            direction="down",  # Dropdown menu direction
            pad={"r": 10, "t": 10},  # Padding settings
            showactive=True,  # Show active button
            x=0.2,  # x-coordinate of the menu
            xanchor="left",  # Anchor point for x-coordinate
            y=button_layer_1_height,  # y-coordinate of the menu
            yanchor="top"  # Anchor point for y-coordinate
        ),
        dict(
            buttons=list([  # List of buttons for changing plot type
                dict(
                    args=["type", "line"],  # Argument for changing plot type to line
                    label="Line Plot",  # Label for the button
                    method="restyle"  # Action method for the button
                ),
                dict(
                    args=["type", "bar"],  # Argument for changing plot type to bar
                    label="Bar Plot",  # Label for the button
                    method="restyle"  # Action method for the button
                )
            ]),
            direction="down",  # Dropdown menu direction
            pad={"r": 10, "t": 10},  # Padding settings
            showactive=True,  # Show active button
            x=0.47,  # x-coordinate of the menu
            xanchor="left",  # Anchor point for x-coordinate
            y=button_layer_1_height,  # y-coordinate of the menu
            yanchor="top"  # Anchor point for y-coordinate
        ),
    ]
)

# Update layout with annotations
fig.update_layout(
    annotations=[
        dict(text="Metrics", x=0.17, xref="paper", y=1.035, yref="paper",  # Annotation for 'Metrics' dropdown
             align="left", showarrow=False),
        dict(text="Plot Style", x=0.42, xref="paper", y=1.035,  # Annotation for 'Plot Style' dropdown
             yref="paper", showarrow=False)
    ]
)

# Update y-axes settings 
fig.update_yaxes(matches=None) # Allow y-axes to have different ranges 
fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True)) 

# Show tick labels for all y-axes# Show the plot 
fig.show()

```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_10.png)

**ROAS performance by audience:**

<details class="details-bar">
  <summary>Click to see: script to output visualization of ROAS performance by audience</summary>

```
import plotly.express as px

#Can customize to include campaign / Advertiser
# Define metric and dimension variables 
metric = "ROAS" # Metric to visualize 
dimension = 'event_dt_utc' # Dimension to group data by

# Group the DataFrame 'df_analysis' by the specified dimension and 'segment', summing up all other numerical columns, and then reset the index 
df_analysis4 = df_analysis.groupby([dimension, 'segment']).sum().reset_index() 

# Calculate ROAS (Return on Advertising Spend)
df_analysis4['ROAS'] = df_analysis4['total_sales']/df_analysis4['total_spend']

# Convert 'event_dt_utc' column to datetime format if the dimension is 'event_dt_utc'
if dimention == 'event_dt_utc':
    df_analysis4['event_dt_utc']= pd.to_datetime(df_analysis4['event_dt_utc'])

# Sort the DataFrame by the specified dimension
df_analysis4 = df_analysis4.sort_values(by=[dimention])

# Display Markdown header indicating the performance metric
mr.Markdown(f'# {metric} ROAS Performance by Audience ')

# Create a scatter plot using Plotly Express
fig = px.scatter(df_analysis4, 
    x=dimention, 
    y=metric, 
    facet_col="segment",
    trendline='ols',
    facet_col_wrap=2,
    height=1000, 
    trendline_color_override="red")

# Update y-axes settings
fig.update_yaxes(matches=None)  # Allow y-axes to have different ranges
fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True)) #  Show tick labels for all y-axes
fig.show()
```

</details>

**ROAS by campaign name and audience segment

<details class="details-bar">
  <summary>Click to see: script to visualize ROAS by campaign name and audience segment</summary>

```
from plotly.subplots import make_subplots

# Define metric and dimension variables
metric = "ROAS" # Metric to visualize
dimention = 'campaign_name' # Dimension to group data by 

# Group the DataFrame 'df_analysis' by the specified dimension and 'segment', summing up all other numerical columns, and then reset the index
df_analysis3 = df_analysis.groupby([dimention,'segment']).sum().reset_index()

# Calculate ROAS (Return on Advertising Spend)
df_analysis3['ROAS'] = df_analysis3['total_sales']/df_analysis3['total_spend']

# Convert 'event_dt_utc' column to datetime format if the dimension is 'event_dt_utc'
if dimention == 'event_dt_utc':
    df_analysis3['event_dt_utc']= pd.to_datetime(df_analysis4['event_dt_utc'])

# Sort the DataFrame by the specified dimension
df_analysis3 = df_analysis3.sort_values(by=[dimention])

# Create a bar plot using Plotly Express
fig = px.bar(df_analysis3, # DataFrame to use
              x = dimention, # X-axis values from the specified dimension column
              y = metric, # Y-axis values from the specified metric column
              color=df_analysis3['segment'], # Color bars by 'segment'
              facet_col=df_analysis3['segment'], # Arrange subplots into columns based on 'segment'
              facet_col_spacing=0.04, # Spacing between subplot columns
              facet_col_wrap=2,height=1000, title=metric)

# Update layout settings
fig.update_layout(
    updatemenus=[ # Define update menus for changing plot type
        dict(
        buttons=list([ # List of buttons for changing plot type
            dict(
            args=["type", "bar"], # Argument for changing plot type to bar
            label="Bar Plot", # Label for the button
            method="restyle" # Action method for the button
            ),
            dict(
            args=["type", "scatter"], # Argument for changing plot type to scatter
            label="Scatter Plot", # Label for the button
            method="restyle" # Action method for the button
            )
            ]),
            direction="down", # Dropdown menu direction
            ),
          ],
      title = f"{metric} vs {dimension} by Audience Segment" # Update figure title
)

# Update y-axes settings
fig.update_yaxes(matches=None) # Allow y-axes to have different ranges
fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True)) # Show tick labels for all y-axes
fig.show()

```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_12.png)

### Action

Based on the results of the analysis and the monitored refresh rate, spend can be adjusted mid-campaign to move funding from low-performing, non-updating segments to higher-performing audiences.

## Post campaign

### Measure overall campaign performance and gather insights

Following the campaign, dashboards will be deployed to measure overall performance volume and ROAS from the generated audiences to help understand which groups did the most work to drive the performance goals. This will be done through a similar process as mid-campaign where a workflow will be defined to manage data flow from AMC to the data lake. The metrics will be focused on a pure ROAS of the audiences and the total volume.

You can utilize the same audience optimization query as before, since each row is broken out by the campaign level and is scheduled to update on a weekly cadence. Following the campaign, users will need to wait for the attribution window to complete post campaign before they can calculate ROAS.

The view below provides a list of the top-performing audience by metric in terms of total sales and total spend and ROAS.

**Top performing audience by metric**

<details class="details-bar">
  <summary>Click to see: script for top performing audience by metric</summary>

```
 #Top Performing Audience by Metric

# Define Markdown content indicating the purpose of the analysis
mr.Markdown('# Top Performing Audience by Metric')

# Define the metric(s) and breakdown dimension(s) to analyze
metric = ['ROAS'] # Metric(s) to analyze
breakdown = ['advertiser_name','campaign_name','segment'] # Breakdown dimension(s)

# Define a function to find the top performer(s) based on the given metric(s) and breakdown dimension(s)
def top_performer(metric,breakdown):
     # Check if 'ROAS' metric is included in the metric list
    if 'ROAS' in metric:
     # Group the DataFrame 'df_analysis' by the breakdown dimensions,
      # summing up 'total_sales' and 'total_spend', and then calculate ROAS
           df_analysis3 = df_analysis[breakdown+[
            'total_sales',
            'total_spend'
            ]].groupby(breakdown).sum().reset_index()
        df_analysis3['ROAS'] = df_analysis3['total_sales']
            /df_analysis3['total_spend']
    else:
     # Group the DataFrame 'df_analysis' by the breakdown dimensions and sum up the specified metric(s)
        df_analysis3 = df_analysis[breakdown+metric]
            .groupby(breakdown).sum().reset_index()

     # Sort the DataFrame by the specified metric(s) in descending order and group by breakdown dimensions
    return df_analysis3.sort_values(metric,ascending=False).groupby(breakdown).sum()

# Call the 'top_performer' function to find the top performing audience(s) based on the given metric(s) and breakdown dimension(s)
top_performer(metric,breakdown)
```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_13.png)

The action this can inspire is to use ROAS as the KPI to understand the efficiency of an audience in activating a campaign.

**Overall volume of performance of each campaign**

Next, we will look at the overall volume of performance of each campaign with a line item at the bottom with the average performance of all audiences.

<details class="details-bar">
  <summary>Click to see: script for overall volume of performance of each campaign</summary>

```
# Define Markdown content indicating the purpose of the analysis
mr.Markdown('# Overall Audience Performance')

# Define the metric(s) and breakdown dimension(s) to analyze
metric = ['ROAS'] # Metric(s) to analyze
breakdown = ['segment'] # Breakdown dimension(s)

# Define a function to find the top performer(s) based on the given metric(s) and breakdown dimension(s)
def top_performer(metric,breakdown):
    # Check if 'ROAS' metric is included in the metric list
    if 'ROAS' in metric:
         # Group the DataFrame 'df_analysis' by the breakdown dimensions,
         # summing up various metrics related to audience performance, and then calculate ROAS
        df_analysis3 = df_analysis[breakdown+[
            'total_sales',
            'total_spend',
            'impressions',
            'total_dpv',
            'total_atc',
            'total_purchases'
            ]].groupby(breakdown).sum().reset_index()
        df_analysis3['ROAS'] = df_analysis3['total_sales']
            /df_analysis3['total_spend']
    else:
        # Group the DataFrame 'df_analysis' by the breakdown dimensions and sum up the specified metric(s)
        df_analysis3 = df_analysis[breakdown+metric]
            .groupby(breakdown).sum().reset_index()

    # Sort the DataFrame by the specified metric(s) in descending order and group by breakdown dimensions
    return df_analysis3.sort_values(metric,ascending=False).groupby(breakdown).sum()
  
# Call the 'top_performer' function to find the overall performance of the audience(s) based on the given metric(s) and breakdown dimension(s)  
df_analysis3 = top_performer(metric,breakdown)

# Calculate the mean of each metric for all audience segments and add it as a row to the DataFrame
df_analysis3.loc['mean'] = df_analysis3.mean()
df_analysis3

```

</details>

![image](/_images/amazon-marketing-cloud/playbooks/paf/paf_14.png)

Here, we can see that the “AMC PlaybookAsinAudience-ASIN5” had the highest volume of sales while “AMC PlaybookAsinAudiences-ASIN2” didn’t drive as high of a volume of sales, despite having more purchases, Add to Carts and detail page views than any other audience.

### Action

Based on the results of the above analysis, the ASIN 4 and 5 audiences could be prioritized, with an additional rate of spend; while ASIN 1 audience showing the lowest rate of ROAS could be deprioritized . Additionally, ASIN 2 showing a higher rate of detail page views and add to cart, an action step for them would be to explore more lower funnel messaging or target them with another ASIN to see if they maintain the same level of engagement while driving downstream purchases.

This playbook can be replicated to maintain a constant stream of measuring audience performance that can create a feedback loop of audience optimizations for ongoing campaigns.

## Next steps

Some options for next steps after the completion of post campaign are:

* To continue to run campaigns on high-performing custom AMC audiences.
* To experiment again for a different use case.Some example of use cases could be:
  * Users who viewed reviews and did not checkout.
  * Users who purchased product A and are targeted with  product B, C, D etc.
* Run experiment with different ASINs

> [TIP Recommendation] DynamoDB would be a more stable store of audience details and configurations. That requires additional permissioning and  cost.
 Integrations with the DSP can be added for automated  deployment.



